{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples_per_class = 1000\n",
    "\n",
    "negative_samples = np.random.multivariate_normal(\n",
    "    mean=[0, 3],\n",
    "    cov=[[1, 0.5],[0.5, 1]],\n",
    "    size=num_samples_per_class)\n",
    "\n",
    "positive_samples = np.random.multivariate_normal(\n",
    "    mean=[3, 0],\n",
    "    cov=[[1, 0.5],[0.5, 1]],\n",
    "    size=num_samples_per_class)\n",
    "\n",
    "inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32) \n",
    "\n",
    "targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype=\"float32\"), np.ones((num_samples_per_class, 1), dtype=\"float32\")))\n",
    "\n",
    "plt.scatter(inputs[:, 0], inputs[:, 1], c=targets[:, 0]) \n",
    "plt.show() # shows targets \n",
    "\n",
    "# weights \n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "W = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim))) \n",
    "b = tf.Variable(initial_value=tf.zeros(shape=(output_dim,)))\n",
    "\n",
    "def model(inputs):\n",
    "  return tf.matmul(inputs, W) + b \n",
    "\n",
    "# loss function\n",
    "def square_loss(targets, predictions):\n",
    "  per_sample_losses = tf.square(targets - predictions) \n",
    "  return tf.reduce_mean(per_sample_losses) \n",
    "\n",
    "# training step function \n",
    "learning_rate = 0.1\n",
    "\n",
    "def training_step(inputs, targets): \n",
    "  with tf.GradientTape() as tape: \n",
    "    predictions = model(inputs)\n",
    "    loss = square_loss(targets, predictions)\n",
    "  grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b]) # find gradient for both W and b, for both tensors \n",
    "  W.assign_sub(grad_loss_wrt_W * learning_rate) # we take that direction, multiply by learning rate, and minus it from the tensor, this updates the weights of this layer \n",
    "  b.assign_sub(grad_loss_wrt_b * learning_rate)\n",
    "\n",
    "  return loss\n",
    "\n",
    "for step in range(40):\n",
    "  loss = training_step(inputs, targets) \n",
    "#   print(f\"Loss at step {step}: {loss:.4f}\") # this shows us the loss for 40 iteratons. We can see it slows down to around 0.027\n",
    "\n",
    "\n",
    "# preidctions (naive way)\n",
    "predictions = model(inputs)\n",
    "plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5) # points are \"0\" if prediction value is below 0.5 and \"1\" if its above 0.5\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
