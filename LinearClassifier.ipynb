{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m num_samples_per_class \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m      7\u001b[0m negative_samples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mmultivariate_normal(\n\u001b[1;32m      8\u001b[0m     mean\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m],\n\u001b[1;32m      9\u001b[0m     cov\u001b[39m=\u001b[39m[[\u001b[39m1\u001b[39m, \u001b[39m0.5\u001b[39m],[\u001b[39m0.5\u001b[39m, \u001b[39m1\u001b[39m]],\n\u001b[1;32m     10\u001b[0m     size\u001b[39m=\u001b[39mnum_samples_per_class)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples_per_class = 1000\n",
    "\n",
    "negative_samples = np.random.multivariate_normal(\n",
    "    mean=[0, 3],\n",
    "    cov=[[1, 0.5],[0.5, 1]],\n",
    "    size=num_samples_per_class)\n",
    "\n",
    "positive_samples = np.random.multivariate_normal(\n",
    "    mean=[3, 0],\n",
    "    cov=[[1, 0.5],[0.5, 1]],\n",
    "    size=num_samples_per_class)\n",
    "\n",
    "# the above generates 2 point clouds, 2 classes of random points in a 2D plan. Both are arrays of the form (1000,2)\n",
    "\n",
    "# stacking together yields a (2000, 2) array \n",
    "inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32) \n",
    "\n",
    "targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype=\"float32\"), np.ones((num_samples_per_class, 1), dtype=\"float32\"))) # 2 classes of points (can represent as class 0 or class 1)\n",
    "# as we've stacked all of class 0 , then class 1, when crteating the sample data, expected output is all of first class, then second, thats what targets is\n",
    "\n",
    "plt.scatter(inputs[:, 0], inputs[:, 1], c=targets[:, 0]) \n",
    "plt.show()\n",
    "# below is the synthetic data we have generated \n",
    "\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "W = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim))) # remember, from tensor operation with w, this layer (one in this example) transforming tensor of rank 2 to rank 1\n",
    "# initally, has random values -> will be altered as we do back propgation \n",
    "b = tf.Variable(initial_value=tf.zeros(shape=(output_dim,)))\n",
    "\n",
    "# forward pass function, the one that does -> prediction = W * input + b\n",
    "def model(inputs):\n",
    "  return tf.matmul(inputs, W) + b # inputs are (x,y). So inputs is 1x2 and W is 2x1. So, output is a scalar (so is b), which is what we want\n",
    "\n",
    "# loss function\n",
    "def square_loss(targets, predictions):\n",
    "  per_sample_losses = tf.square(targets - predictions) \n",
    "  return tf.reduce_mean(per_sample_losses) # We need to average these per-sample loss scores into a single scalar loss value: this is what reduce_mean does.\n",
    "\n",
    "# training step function \n",
    "learning_rate = 0.1\n",
    "\n",
    "def training_step(inputs, targets): \n",
    "  with tf.GradientTape() as tape: \n",
    "    predictions = model(inputs)\n",
    "    loss = square_loss(targets, predictions)\n",
    "  grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b]) # find gradient for both W and b, for both tensors \n",
    "  W.assign_sub(grad_loss_wrt_W * learning_rate) # we take that direction, multiply by learning rate, and minus it from the tensor, this updates the weights of this layer \n",
    "  b.assign_sub(grad_loss_wrt_b * learning_rate)\n",
    "\n",
    "  return loss\n",
    "\n",
    "for step in range(40):\n",
    "  loss = training_step(inputs, targets) \n",
    "#   print(f\"Loss at step {step}: {loss:.4f}\") # this shows us the loss for 40 iteratons. We can see it slows down to around 0.027\n",
    "\n",
    "\n",
    "# lets see what the \n",
    "predictions = model(inputs)\n",
    "plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5) # points are \"0\" if prediction value is below 0.5 and \"1\" if its above 0.5\n",
    "plt.show()\n",
    "\n",
    "# done\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
